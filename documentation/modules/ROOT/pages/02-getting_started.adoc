= Getting Started
include::_attributes.adoc[]
:profile: acs

We will install ACS thought three main options and afterwards we will install an small demo that will serve examples to our workshop.

NOTE: You can request in RHPDS the environment already installed (including the demo) - Go to Multi-Product Demo -> Openshift 4 Advanced Cluster Security 3

The three options depicted are:

* Option 1 - ACS Operator Install Manually (recommended to learn the process)
* Option 2 - Deploying ACS with Ansible (if you're in a rush and/or love Ansible)
* Option 3 - Deploying ACS with Openshift GitOps (the GitOps way!)

IMPORTANT: Independently on how you're installing, please deploy the deploy in the last step of ACS Deploy Demo to complete this module.

[#install_acs_operator]
== ACS Operator Install - Option 1

Find and the Advanced Cluster Security Operator from the Operator Hub.

image::install/00_operator_hub.png[ACS Operator 1, 800]

Install the selected operator by clicking on the ``Install`` button.

image::install/01_select_acs_operator.png[ACS Operator 2, 800]

Confirm default installation parameters (auto update, latest channel, ``openshift-operators`` namespace).

image::install/02_install_acs_operator.png[ACS Operator 3, 800]

Wait for completion, the installation will take a few seconds.

image::install/03_wait_for_completion.png[ACS Operator 4, 800]

Access the now ready operator by clicking on the ``View Operator`` button.

image::install/04_operator_ready.png[ACS Operator 5, 800]

[#install_acs_central]
== ACS Central Cluster Installation

In this section we will deploy the Central component in the lab cluster. The Central is made up two main deployments:

* The ``central`` service, which exposes api and console and communicates with Sensors on secured clusters.

* The ``scanner`` service, which has the role of scanning the deployed pods images.

Create a new ``stackrox`` namespace (using the web console or the cli as follows). We will install our components here.

[.console-input]
[source,bash,subs="attributes+,+macros"]	
----	
oc new-project stackrox	
----

The following is an example of the central custom resource. You can do this in two ways, via the web console or via the CLI.

**With Web Console:**

From the operator ready screen by clicking on ``View Operator`` or browsing from the Installed Operator menu, you can reach the ACS' operator intrface:

image::install/04_1_create_central_resource.png[ACS Operator 51, 800]

Check the ``YAML`` radio button and paste the ``Central`` CR you see below.

image::install/04_2_create_central_resource.png[ACS Operator 52, 800]

You can check the progress by switching to the `Development` perspective, in the `Topology` menu.

image::install/04_3_create_central_resource.png[ACS Operator 53, 800]


**With OC client:**

[.console-input]
[source,yaml,subs="attributes+,+macros"]	
----	
apiVersion: platform.stackrox.io/v1alpha1
kind: Central
metadata:
  name: stackrox-central-services
  namespace: stackrox
spec:
  central:
    exposure:
      loadBalancer:
        enabled: false
        port: 443
      nodePort:
        enabled: false
      route:
        enabled: true
    persistence:
      persistentVolumeClaim:
        claimName: stackrox-db
  egress:
    connectivityPolicy: Online
  scanner:
    analyzer:
      scaling:
        autoScaling: Enabled
        maxReplicas: 5
        minReplicas: 2
        replicas: 3
    scannerComponent: Enabled	
----

Create the ``central`` custom resource using the template file provided in this repository.

[.console-input]
[source,bash,subs="attributes+,+macros"]	
----	
oc apply -f stackrox-central-services.yaml -n stackrox
----

Monitor the installation using the watch option:

[.console-input]
[source,bash,subs="attributes+,+macros"]	
----	
oc get pods -n stackrox -w
----

Once the installation is complete obtain the generated admin password from the ``central-htpasswd`` secret.

[.console-input]
[source,bash,subs="attributes+,+macros"]	
----	
oc -n stackrox get secret central-htpasswd -o go-template='{{index .data "password" | base64decode}}'
----

Extract the hostname of the generated route.

[.console-input]
[source,bash,subs="attributes+,+macros"]	
----	
oc get routes/central -n stackrox -o jsonpath='{.spec.host}'
----

Login to https://<route_hostname> using the ``admin`` username and the password extracted before.

image::install/05_login.png[ACS Operator 6, 800]

[#config_acs_securedcluster]
== ACS Secured Cluster Configuration

To join a cluster to ACS it is necessary to generate a cluster init bundle containing TLS secrets for Sensor, Collectors and Admission Controllers.

[#config_acs_securedcluster_init_bundle]
=== Generating an init bundle by using the RHACS portal

. Generate the cluster init bundle by accessing the ``Integration`` subsection in the ``Platform Configuration`` section 
+
image::install/06_acs_integrations.png[ACS Operator 7, 800]

. Generate the bundle with a unique cluster name, in our case, ``demo-cluster``
+
image::install/07_generate_cluster_init_bundle.png[ACS Operator 8, 800]

. Download the cluster init bundle secret.
+
image::install/08_download_cluster_init_bundle_secret.png[ACS Operator 9, 800]

. Apply the cluster init bundle secret on the target secured cluster
+
[.console-input]
[source,bash,subs="attributes+,+macros"]	
----	
oc apply -f ~/Downloads/demo-cluster-cluster-init-secrets.yaml -n stackrox
----

[#config_acs_securedcluster_install_scs]
=== Installing secured cluster services

NOTE: This workshop uses the same cluster as central and secured cluster. In a real time scenario there will be many different secured clusters. Please ensure to install the ACS Operator in all the secured cluster in order to manage the SecuredCluster CR.

The ``SecuredCluster`` custom resource is quite simple. The following example shows the configuration for a ``demo-cluster`` target. Notice the ``collector`` configuration, with the collection method set to ``KernelModule``. The alternative collection approach would be ``eBPF``. The ``TolerateTaints`` lets the Collector daemonset be deployed also on nodes with special taints, like the ODF nodes.

[.console-input]
[source,yaml,subs="attributes+,+macros"]	
----	
apiVersion: platform.stackrox.io/v1alpha1
kind: SecuredCluster
metadata:
  name: stackrox-secured-cluster-services
  namespace: stackrox
spec:
  admissionControl:
    listenOnCreates: true
    listenOnEvents: true
    listenOnUpdates: true
  clusterName: demo-cluster
  perNode:
    collector:
      collection: KernelModule
      imageFlavor: Regular
    taintToleration: TolerateTaints
----

NOTE: Check the settings of the https://docs.openshift.com/acs/installing/install-ocp-operator.html#addmission-controller-settings_install-ocp-operator[SecuredCluster operator documentation] for more information.

. Create the Secured Cluster custom Resource using (and optionally custumizing) the example provided in the repository.
+
[.console-input]
[source,bash,subs="attributes+,+macros"]	
----	
oc apply -f stackrox-secured-cluster-services.yaml -n stackrox
----
+
Or using the web console, in the ACS opertator view, as follows:app-name:
+
.. Under the Provided APIs section, select Create instance on the Secured Cluster API
+
image::install/09_create_secured_cluster_resource.png[ACS Operator 9, 800]

.. And then copy & paste the yaml content
+
image::install/10_create_secured_cluster_resource_yaml.png[ACS Operator 10, 800]


. Monitor the installation using the watch option (or using the web console ``Topology`` view from the ``Development`` perspective as mentioned before):
+
[.console-input]
[source,bash,subs="attributes+,+macros"]	
----	
oc get pods -n stackrox -w
----

. At the end of the installation, go to the central console and check the correct attachment of the secured cluster. 
+
image::install/11_verify_cluster_list.png[ACS Operator 11, 800]

[#deploy_acs_ansible]
== Deploying ACS with Ansible - Option 2

IMPORTANT: Do this step ONLY if you have not installed the Operator manually neither selected the acs-gitops installation.

[.console-input]
[source,bash,subs="attributes+,+macros"]	
----
ansible-galaxy collection install kubernetes.core	
git clone https://github.com/rcarrata/rhacs-demo.git rhacs-demo
cd rhacs-demo
----	

Follow the https://github.com/rcarrata/rhacs-demo#rhacs-installyaml-file-example[instructions] if you need to customize the ACS installation (change passwords, etc)

[.console-input]
[source,bash,subs="attributes+,+macros"]	
----
ansible-playbook rhacs-install.yaml
----

[#deploy_acs_gitops]
== Deploying ACS with Openshift GitOps - Option 3

Install ACS using Openshift GitOps:

[.console-input]
[source,bash,subs="attributes+,+macros"]	
----
git clone https://github.com/rcarrata/acs-gitops.git
cd acs-gitops
----

Install bootstrap Openshift GitOps / ArgoCD

[.console-input]
[source,bash,subs="attributes+,+macros"]	
----
until oc apply -k bootstrap/base/; do sleep 2; done
oc apply -k acs-deploy/applications
----

TODO: Add an image of the bootstrap

[#deploy_demo_acs] 
== Deploying Demo in ACS - Mandatory

IMPORTANT: Independent of which option to install you used, you need to deploy the ACS Demo into your cluster.

Download the repo with the demo:

[.console-input]
[source,bash,subs="attributes+,+macros"]	
----
ansible-galaxy collection install kubernetes.core
git clone https://github.com/rcarrata/rhacs-demo
cd rhacs-demo
----

Apply the ansible demo into the cluster:
[.console-input]
[source,bash,subs="attributes+,+macros"]	
----
ansible-playbook rhacs-demo.yaml
----

After the Playbook execution (and if everything worked properly), the output will be the following:
[.console-output]
[source,bash,subs="attributes+,+macros"]	
----	
TASK [ocp4_workload_stackrox_demo_apps : post_workload tasks complete] ********************************************************
ok: [localhost] => {
    "msg": "Post-Workload Tasks completed successfully."
}
----

